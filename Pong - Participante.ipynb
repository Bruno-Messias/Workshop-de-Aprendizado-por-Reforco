{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Workshop de Introdu√ß√£o ao Aprendizado por Refor√ßo\n",
    "‚†Ä\n",
    "\n",
    "Bem vindes ao **Workshop de Introdu√ß√£o ao Aprendizado por Refor√ßo**, organizado pelo Grupo Turing! \n",
    "\n",
    "O objetivo deste evento √© ensinar o b√°sico necess√°rio da √°rea de Aprendizado por Refor√ßo utilizando um dos maiores cl√°ssicos da hist√≥ria dos video-games: ***Pong***.\n",
    "\n",
    "![Pong](https://media2.giphy.com/media/aTGwuEFyg6d8c/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèì Sobre o Pong\n",
    "\n",
    "Come√ßaremos falando sobre o problema, ou seja, sobre o jogo Pong. Este que foi o primeiro jogo de video-game lucrativo da hist√≥ria, publicado em 1972, constando 48 anos de legado.\n",
    "\n",
    "Pong simula uma partida de t√™nis, existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes √© n√£o somente evitar que a bola passe por ela, como tamb√©m fazer com que esta passe pela linha que a outra raquete protege, criando assim a premissa que sustenta o interesse pelo jogo. Queremos ent√£o desenvolver um algoritmo capaz de - sem nenhuma explica√ß√£o adicional - maximizar as suas recompensas, sendo as a√ß√µes, os estados e as recompensas, todas relativas ao jogo Pong. Teremos no final, portanto, um modelo treinado capaz de bom desempenho dentro do ambiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Programando..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o Gym\n",
    "\n",
    "O **[Gym](https://gym.openai.com/)** √© uma biblioteca desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizagem por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
    "<figcaption>Exemplo de Ambientes do Gym</figcaption>\n",
    "<br>\n",
    "\n",
    "Para se ter acesso a esses ambientes, basta importar o Gym da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que √© um Ambiente?\n",
    "\n",
    "Um **Ambiente** de Aprendizagem por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.\n",
    "\n",
    "Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.\n",
    "\n",
    "<img src=\"https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif\" alt=\"Uma A√ß√£o do Xadrez\" class=\"inline\"/>\n",
    "\n",
    "### Como Funciona um Ambiente do Gym?\n",
    "\n",
    "Agora que voc√™ j√° sabe o que √© um ambiente, √© preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns m√©todos simples para facilitar a comunica√ß√£o com eles:\n",
    "\n",
    "<br>\n",
    "\n",
    "| M√©todo               | Funcionalidade                                          |\n",
    "| :------------------- |:------------------------------------------------------- |\n",
    "| reset()              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |\n",
    "| step(action)         | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |\n",
    "| render()             | Renderiza o ambiente                                    |\n",
    "| close()              | Fecha o ambiente                                        |\n",
    "\n",
    "<br>\n",
    "\n",
    "Assim, o c√≥digo para interagir com o ambiente costuma seguir o seguinte modelo:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "ambiente = gym.make(\"Nome do Ambiente\")                         # Cria o ambiente\n",
    "observa√ß√£o = ambiente.reset()                                   # Inicializa o ambiente\n",
    "acabou = False\n",
    "\n",
    "while not acabou:\n",
    "    ambiente.render()                                           # Renderiza o ambiente\n",
    "    observa√ß√£o, recompensa, acabou, info = ambiente.step(a√ß√£o)  # Executa uma a√ß√£o\n",
    "    \n",
    "ambiente.close()                                                # Fecha o ambiente\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Ambiente\n",
    "\n",
    "Para utilizar um dos ambientes do Gym, n√≥s utilizamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando seu valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambiente pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"pong:turing-easy-v0\")\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Trocar para Pong\n",
    "\n",
    "Nesse caso, n√≥s vamos utilizar o ambiente ```CartPole-v1```, um ambiente bem simples que modela um p√™ndulo invertido em cima de um carrinho buscando seu estado de equil√≠brio.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*jLj9SYWI7e6RElIsI3DFjg.gif\" width=\"400px\" alt=\"Ambiente do CartPole-v1\" class=\"inline\"/>\n",
    "\n",
    "#### CartPole\n",
    "\n",
    "Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.\n",
    "\n",
    "O **Espa√ßo de Observa√ß√£o** do CartPole √© definido por 4 informa√ß√µes:\n",
    "\n",
    "<br>\n",
    "\n",
    "|     | Informa√ß√£o                         | Min     | Max    |\n",
    "| :-- | :--------------------------------- | :-----: | :----: |\n",
    "| 0   | Posi√ß√£o do Carrinho                | -4.8    | 4.8    |\n",
    "| 1   | Velocidade do Carrinho             | -Inf    | Inf    |\n",
    "| 2   | √Çngulo da Barra                    | -24 deg | 24 deg |\n",
    "| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |\n",
    "\n",
    "<br>\n",
    "\n",
    "Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.4226079e+00  1.6426810e+38  2.2506310e-01 -1.2768003e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J√° o **Espa√ßo de A√ß√£o** √© composto por duas a√ß√µes √∫nicas: mover o carrinho para a **esquerda** ou para a **direita**.\n",
    "\n",
    "Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos mov√™-lo para a direita, enviamos um `env.step(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa fun√ß√£o deve rodar um episodio de Pong escolhendo a√ß√µes aleat√≥rias\n",
    "def rodar_ambiente():\n",
    "    # Criando o ambiente 'pong:turing-easy-v0'\n",
    "    env = gym.make(\"pong:turing-easy-v0\")\n",
    "\n",
    "    # Resete o ambiente e receba o primeiro estado\n",
    "    state = ...\n",
    "\n",
    "    # Inicializando done como false\n",
    "    done = False\n",
    "\n",
    "    # Loop de treino\n",
    "    while not done:\n",
    "        # Escolha uma acao aleatoria\n",
    "        action = ...\n",
    "\n",
    "        # Tome essa acao e receba as informacoes do estado seguinte\n",
    "        next_state, reward, done, info = ...\n",
    "\n",
    "        # Renderize o ambiente\n",
    "        ...\n",
    "\n",
    "        # Atualizando o estado\n",
    "        state = next_state\n",
    "\n",
    "    # Fechando o ambiente\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando a fun√ß√£o\n",
    "rodar_ambiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë©‚Äçüíª Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, precisaremos utilizar uma biblioteca chamada ***Numpy*** para auxiliar nas computa√ß√µes. Esta √© uma biblioteca do Python capaz de manusear diversas computa√ß√µes matem√°ticas com maestria e ser√° importante futuramente para o nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, √© importante definir nossas _vari√°veis globais_, as quais s√£o respons√°veis por regir valores que podem ser ajustados para melhor desempenho do modelo, ou maior velocidade de treinamento. Algumas dessas vari√°veis apenas far√£o sentido mais a frente, mas tentaremos dar explica√ß√µes sucintas para cada uma delas.\n",
    "\n",
    "* \"EPSILON\" ser√° o a vari√°vel correspondete √† letra grega \"$\\epsilon$\" de mesmo nome, que √© a nota√ß√£o usual para a probabilidade de explora√ß√£o do nosso agente. Para exemplificar a posi√ß√£o do nosso agente, que iluminar√° a fun√ß√£o dessa vari√°vel, ser√° dado um exemplo do cotidiano humano: Digamos que sua m√£e lhe concede algumas moedas de um real para comprar lanches na escola. Sua escola disp√µe de quatro m√°quinas de alimentos, mas todas elas possuem diferentes probabilidades de lhe conceder chocolates de marcas estranhas das quais voc√™ nunca ouviu falar. Como voc√™ deve aumentar a sua satisfa√ß√£o (conseguir mais frequentemente os chocolates mais gostosos) com um n√∫mero finito de recursos (as moedas que voc√™ possui) sem nunca saber com certeza o que esperar do seu investimento. Esse problema √© famoso na √°rea e √© conhecido como _the multi-armed bandit problem_, ent√£o como resolv√™-lo? Usaremos o algoritmo $\\epsilon$-guloso, o qual nos instrui a inicialmente explorar bastante at√© existir certo conhecimento do que esperar de cada m√°quina de chocolate e, conforme √© adquirida experi√™ncia, certa confi√¢ncia de que determinada m√°quina nos fornece com maior frequencia os chocolates de melhor gosto √© criada, ent√£o a necessidade de explorar ir√° decrescer. A vari√°vel em quest√£o, portanto, representa a probabilidade inicial de tentarmos uma a√ß√£o aleat√≥ria.\n",
    "\n",
    "* \"EPSILON_MIN\", conforme nossa confian√ßa aumenta, ser√° desej√°vel aproveitar nosso conhecimento com maior frequencia do que explorar ainda mais o ambiente. Contudo, sempre far√° sentido explorar ao menos um pouco enquanto treinamos o algoritmo e, portanto, √© criado um limite inferior para a probabilida de explora√ß√£o e tal √© a fun√ß√£o dessa vari√°vel.\n",
    "\n",
    "* \"DECAIMENTO\", como explicado, √© desejado diminuir a explora√ß√£o conforme a experi√™ncia aumenta. Essa vari√°vel √© justamente a taxa de decaimento da nossa probabilidade de explora√ß√£o.\n",
    "\n",
    "* \"ALFA\", algoritmos de aprendizado de m√°quina costumam precisar de uma forma de serem otimizados. Q-learning trabalha em cima de gradientes, uma entidade matem√°tica que indica a dire√ß√£o para maximizar (ou minimizar) uma fun√ß√£o. Dispondo dessa dire√ß√£o, precisamos informar qual deve ser o tamanho do passo a ser dado antes de atualizar a nova \"dire√ß√£o ideal\".\n",
    "\n",
    "* \"GAMA\" √© a vari√°vel correspondente a letra grega de mesmo nome \"$\\gamma$\", a qual denota o quanto desejamos que nosso algoritmo considere eventos futuros. Se \"$\\gamma = 1$\", nosso algoritmo avaliar√° que a situa√ß√£o futura ser melhor que a atual √© t√£o importante quanto a recompensa da situa√ß√£o atual em si, por outro lado, se \"$\\gamma = 0$\", os eventos futuros n√£o apresentam import√¢ncia alguma para nosso algoritmo. \n",
    "\n",
    "* \"N_EPISODIOS\" dita quantas vezes o agente dever√° \"reviver\" o ambiente (vit√≥rias e derrotas) antes de acabar seu treinamento.\n",
    "\n",
    "* \"Q\" √© um dicion√°rio, ou seja, uma estrtura de dados capaz de buscar elementos de forma r√°pida. N√≥s o usaremos para guardar valores relativos √†s estimativas do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes da Pol√≠tica Epsilon Greedy\n",
    "# Epsilon: probabilidade de experimentar uma a√ß√£o aleat√≥ria\n",
    "EPSILON = 0.7        # Valor inicial do epsilon\n",
    "EPSILON_MIN = 0.01   # Valor m√≠nimo de epsilon\n",
    "DECAIMENTO = 0.98    # Fator de deca√≠mento do epsilon (por epis√≥dio)\n",
    "\n",
    "# Hiperpar√¢metros do Q-Learning\n",
    "ALFA = 0.05          # Learning rate\n",
    "GAMA = 0.9           # Fator de desconto\n",
    "\n",
    "N_EPISODIOS = 250    # Quantidade de epis√≥dios que treinaremos\n",
    "\n",
    "# Dicion√°rio dos valores de Q\n",
    "# Chaves: estados; valores: qualidade Q atribuida a cada a√ß√£o\n",
    "Q = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "Antes de partir para o c√≥digo do algoritmo em si, alguns esclarecimentos do ponto de vista do agente sobre o ambiente devem ser feitos. Pong √© um jogo simples, consiste em controlar a sua raquete com esperan√ßa de maximizar a sua pontua√ß√£o e manter a do advers√°rio a menor poss√≠vel. Devemos, contudo, adicionar algum rigor √† essa express√£o, de forma que o algoritmo seja capaz de lidar com essa informa√ß√£o. \"controlar a raquete\" significa haver tr√™s poss√≠veis a√ß√µes: subir, descer ou n√£o se mover. Cada a√ß√£o tomada em cada estado deve nos retornar uma recompensa e deve nos levar a um novo estado. A nossa recompensa ser√° de +500 u.r. se pontuarmos, -500 u.r. se pontuarem sobre n√≥s e 0 caso contr√°rio. Finalmente, os estados ser√£o vetores dist√¢ncia entre a raquete a bola, caso jogado no modo f√°cil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importando a Biblioteca Gym\n",
    "import gym\n",
    "\n",
    "# Criando o nosso Ambiente: Pong\n",
    "env = gym.make(\"pong:turing-easy-v0\")\n",
    "\n",
    "# N√∫mero total de a√ß√µes: 3\n",
    "# 0 = parado; 1 = baixo; 2 = cima\n",
    "n_acoes = env.action_space.n\n",
    "\n",
    "print('N√∫mero de a√ß√µes:', n_acoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como foi dito, os estados que o nosso agente utilizar√° para tomar suas decis√µes s√£o vetores (\"setas\") que apontam da raquete controlada at√© a bola, imagine agora o caso onde esses vetores apresentassem apenas seus m√≥dulos como n√∫meros inteiros. Se a nossa tela possuisse 800 u.d. de largura e 600 u.d. de altura, o espa√ßo amostral dos diferentes vetores e, portanto, diferentes estados do ponto de vista do agente seria $2 \\times 800 \\times 600 = 960000$. Para um algoritmo que consiste em guardar estimativas do valor de cada a√ß√£o para cada estado, esse n√∫mero de estados exigiria n√£o somente guardar como atualizar cada um desses valores at√© chegar numa estimativa otimizada para os 3 milh√µes de valores. N√£o √© o ideal. Para simplificar (e agilizar) a situa√ß√£o, \"discretizar\" os nossos estados √© razo√°vel e esperado, faremos com que estados similares o suficiente sejam considerados como iguais e comparilhem das mesmas estimativas (n√£o faz sentido distinguir o vetor (502,234) do vetor (515,222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiza_estado(estado):\n",
    "    return tuple(round(x/10) for x in estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o processo de de escolha de a√ß√£o, √© necess√°rio lembrar do dilema entre **explora√ß√£o** e **exploita√ß√£o**. O modelo pode explorar o ambiente, realizando novas escolhas que as recompensas naquele instante n√£o indicam como a melhor, mas que podem a resultar em uma recompensa maior, ou o modelo pode aproveitar o conhecimento que j√° possui, de forma a maximizar a recompensa que receber√° no epis√≥dio. Por√©m, √© imposs√≠vel explorar e exploitar em uma mesma a√ß√£o.\n",
    "\n",
    "De forma a assegurar que o agente busque tanto novas alternativas que podem gerar melhores resultados quanto ser capaz de utilizar o aprendizado obtido de forma a maximizar a sua recompensa, existem diversas estrat√©gias para a escolha de explora√ß√£o e exploita√ß√£o. Uma das mais utilizadas, que tamb√©m vamos utilizar aqui, √© a sele√ß√£o de a√ß√µes pelo m√©todo **\"$\\epsilon$-greedy\"**.\n",
    "\n",
    "A estrat√©gia \"$\\epsilon$-greedy\" est√° definida da seguinte forma: √© retirado um n√∫mero aleat√≥rio, no intervalo entre 0 e 1. caso este n√∫mero tenha valor inferior ao valor do epsilon, a escolha ser√° de uma a√ß√£o aleat√≥ria, o que configura explora√ß√£o. Caso este n√∫mero seja superior ao epsilon, a a√ß√£o a ser tomada √© a que gera a maior recompensa de acordo com os valores da tabela Q.\n",
    "\n",
    "Este valor de $\\epsilon$ n√£o √© constante ao longo do treinamento. Inicialmente, este valor √© alto, incentivando a maior explora√ß√£o do ambiente. A medida que o treinamento ocorre, mais informa√ß√£o sobre o ambiente √© adquirida, conseguindo uma tabela Q mais representativa da realidade. Dessa forma, quanto mais avan√ßado no treinamento, menor a necessidade de explora√ß√£o e maior a necessidade de exploitar o conhecimento adquirido para maximizar a recompensa. Esta atualiza√ß√£o do $\\epsilon$ √© chamada **\"$\\epsilon$-decay\"** (decaimento do epsilon)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escolhe_acao(env, Q, estado, epsilon):\n",
    "    # Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "\n",
    "    # Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\n",
    "    # Se esse n√∫mero for menor que epsilon, tomamos uma a√ß√£o aleat√≥ria\n",
    "    if np.random.random() < epsilon:\n",
    "        # Escolhemos uma a√ß√£o aleat√≥ria, com env.action_space.sample()\n",
    "        acao = ...\n",
    "    else:\n",
    "        # Escolhemos a melhor a√ß√£o para o estado atual, com np.argmax()\n",
    "        acao = ...\n",
    "    return acao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar uma partida, s√£o necess√°rias algumas etapas. Inicialmente, o ambiente √© reiniciado, de forma a inicar um novo epis√≥dio. Em seguida, √© necess√°rio discretizar o estado, pelos motivos j√° explicados acima. Esta discretiza√ß√£o deve ocorrer toda vez em que estamos em um novo estado.\n",
    "\n",
    "Enquanto o ambiente n√£o chega em seu estado terminal, indicado pela vari√°vel \"done\", ser√° feito o processo de escolha de a√ß√µes e, uma vez escolhida, deve-se receber do ambiente o pr√≥ximo estado, a recompensa que a a√ß√£o escolhida gerou, al√©m do sinal se estamos no estado terminal. Todo o processo √© repetido novamente para o pr√≥ximo estado, at√© o final do epis√≥dio.\n",
    "\n",
    "Como explicado na se√ß√£o sobre a biblioteca \"Gym\", \"env.render()\" tem como papel mostrar o ambiente (neste caso, a partida de Pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roda_partida(env, Q, renderiza=True):\n",
    "    # Resetamos o ambiente\n",
    "    estado = env.reset()\n",
    "\n",
    "    # Discretizamos o estado\n",
    "    estado = ...\n",
    "    \n",
    "    done = False\n",
    "    retorno = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Escolhemos uma a√ß√£o\n",
    "        acao = ...\n",
    "\n",
    "        # Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\n",
    "        prox_estado, recompensa, done, info = ...\n",
    "\n",
    "        # Discretizamos o pr√≥ximo estado\n",
    "        prox_estado = ...\n",
    "\n",
    "        # Renderizamos o Ambiente\n",
    "        if renderiza:\n",
    "            env.render()\n",
    "\n",
    "        retorno += recompensa\n",
    "        estado = prox_estado\n",
    "\n",
    "    print(f'retorno {retorno:.1f},  '\n",
    "          f'placar {env.score[0]}x{env.score[1]}')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rodamos uma partida de Pong\n",
    "roda_partida(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Treinamento\n",
    "\n",
    "TODO: Falar de Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar Bellman (o b√°sico, j√° foi abordado antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualiza_q(Q, estado, acao, recompensa, prox_estado):\n",
    "    # para cada estado ainda n√£o descoberto, iniciamos seu valor como nulo\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "    if prox_estado not in Q.keys(): Q[prox_estado] = [0] * n_acoes\n",
    "\n",
    "    # equa√ß√£o de Bellman\n",
    "    Q[estado][acao] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar (brevemente?) pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def salva_tabela(Q, nome = 'model.pickle'):\n",
    "    with open(nome, 'wb') as pickle_out:\n",
    "        pickle.dump(Q, pickle_out)\n",
    "\n",
    "def carrega_tabela(nome = 'model.pickle'):\n",
    "    with open(nome, 'rb') as pickle_out:\n",
    "        return pickle.load(pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar fun√ß√£o de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina(env, Q):\n",
    "    retornos = []      # retorno de cada epis√≥dio\n",
    "    epsilon = EPSILON\n",
    "\n",
    "    for episodio in range(1, N_EPISODIOS+1):\n",
    "        # resetar o ambiente e discretizar a a√ß√£o\n",
    "        ...\n",
    "        \n",
    "        done = False\n",
    "        retorno = 0\n",
    "        \n",
    "        while not done:\n",
    "            # escolher uma a√ß√£o\n",
    "            ...\n",
    "\n",
    "            # tomar a a√ß√£o\n",
    "            ...\n",
    "\n",
    "            # discretizar o pr√≥ximo estado\n",
    "            ...\n",
    "\n",
    "            atualiza_q(Q, estado, acao, recompensa, prox_estado)\n",
    "\n",
    "            retorno += recompensa\n",
    "            estado = prox_estado\n",
    "\n",
    "        # calcular o pr√≥ximo epsilon\n",
    "        epsilon = ...\n",
    "        epsilon = max(epsilon, EPSILON_MIN)\n",
    "\n",
    "        retornos.append(retorno)\n",
    "\n",
    "        if episodio % 10 == 0:\n",
    "            salva_tabela(Q)\n",
    "\n",
    "        print(f'epis√≥dio {episodio},  '\n",
    "              f'retorno {retorno:7.1f},  '\n",
    "              f'retorno m√©dio (√∫ltimos 10 epis√≥dios) {np.mean(retornos[-10:]):7.1f},  '\n",
    "              f'placar {env.score[0]}x{env.score[1]},  '\n",
    "              f'epsilon: {epsilon:.3f}')\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "treina(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèì Testando nosso Agente Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roda_partida(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}